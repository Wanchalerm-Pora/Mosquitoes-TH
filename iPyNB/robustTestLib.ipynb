{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf98cb1e-0126-4045-a14d-e77e5590f20d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77447bf4-e10c-4519-9684-bd3606a9a18d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-27 02:06:38.457959: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os, shutil, glob, tempfile, gc\n",
    "import math, random\n",
    "from datetime import datetime\n",
    "import PIL\n",
    "import PIL.Image\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "mos_img_dir = './datasets/Chula7MosDatasets/G-A52s_and_V-Y21/'\n",
    "class_names = ['Ae-aegypti', 'Ae-albopictus', 'Ae-vexans', 'An-tessellatus', 'Cx-quinquefasciatus', 'Cx-vishnui', 'Misc']\n",
    "\n",
    "#NUM_EPOCH_PER_TRAIN = 20\n",
    "#no training\n",
    "\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "# initial value of learning rate\n",
    "#LR = 2e-5\n",
    "# multiplier factor\n",
    "#LR_MULTIPLIER = 0.5\n",
    "# number of epochs per reduction\n",
    "#NUM_EPOCH_PER_LR_UPDATE = 7\n",
    "\n",
    "# original image size\n",
    "IMG_SIZE = 224\n",
    "\n",
    "# ratio of (train, val, test)\n",
    "RATIO = (0.90, 0.10, 0.0) # train, val, test, (test all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b66844f6-dc35-4ce7-be85-d018bb2b9a12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "36cd6efa-214a-416b-adce-a5d2d4aa893b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def between0i1x(f):\n",
    "    if 0 <= f < 1:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "\n",
    "def RandomTrainValTest(src_dir, dst_dir, ratio = (0.8, 0.2, 0)):\n",
    "    #check if Destination directory already exists\n",
    "    if os.path.exists(dst_dir):\n",
    "        print('Destination directory already exists. Do nothing!!!')\n",
    "        return\n",
    "    TRAIN_RATIO = round(ratio[0], 3)\n",
    "    VAL_RATIO = round(ratio[1], 3)\n",
    "    TEST_RATIO = round(1 - TRAIN_RATIO - VAL_RATIO, 3)\n",
    "    if not between0i1x(TEST_RATIO) or not between0i1x(VAL_RATIO) or not between0i1x(TRAIN_RATIO):\n",
    "        print('Ratio must be more than or equal 0 and less than 1.')\n",
    "        return\n",
    "    print('Ratio = (' + str(TRAIN_RATIO) +', ' + str(VAL_RATIO) + ', ' + str(TEST_RATIO) + ').' )\n",
    "    if TEST_RATIO != round(ratio[2],3):\n",
    "        print('Sum of ratio must be 1.')\n",
    "        return\n",
    "    \n",
    "    # create a temp dir and copy all files recursively to temp_dir\n",
    "    tmp = tempfile.TemporaryDirectory()\n",
    "    tmp_dir = tmp.name + '/tmp/'\n",
    "    shutil.copytree(src_dir, tmp_dir)\n",
    "    # scan species, they must be immediatly under the src_dir \n",
    "    for species in os.scandir(src_dir):\n",
    "        if not species.name.startswith('.') and species.is_dir():\n",
    "            class_dir = tmp_dir + '/' + species.name\n",
    "            print('Found ' + species.name)\n",
    "            #calculate No. of files in each species \n",
    "            file_no = 0\n",
    "            for entry2 in os.scandir(class_dir):\n",
    "                if not entry2.name.startswith('.') and entry2.is_file():\n",
    "                    file_no += 1\n",
    "            print('No. of files in ' + species.name + ' = ' + str(file_no) + '.')\n",
    "            # create subdirectories if necessary\n",
    "            if not os.path.exists(dst_dir):\n",
    "                os.mkdir(dst_dir)\n",
    "            test_dir = dst_dir + '/test/' \n",
    "            val_dir = dst_dir + '/val/' \n",
    "            train_dir = dst_dir + '/train/' \n",
    "            if not os.path.exists(test_dir):\n",
    "                os.mkdir(test_dir)\n",
    "            if not os.path.exists(val_dir):\n",
    "                os.mkdir(val_dir)\n",
    "            if not os.path.exists(train_dir):\n",
    "                os.mkdir(train_dir)\n",
    "            # create species.name dir    \n",
    "            stest_dir = test_dir + species.name + '/'\n",
    "            sval_dir = val_dir + species.name + '/'\n",
    "            strain_dir = train_dir + species.name + '/'\n",
    "            if not os.path.exists(stest_dir):\n",
    "                os.mkdir(stest_dir)\n",
    "            if not os.path.exists(sval_dir):\n",
    "                os.mkdir(sval_dir)\n",
    "            if not os.path.exists(strain_dir):\n",
    "                os.mkdir(strain_dir)\n",
    "            # calculate number of files for train, val, test    \n",
    "            no_file_train = math.ceil(file_no*TRAIN_RATIO)\n",
    "            no_file_val = math.ceil(file_no*VAL_RATIO)\n",
    "            no_file_test = math.ceil(file_no*TEST_RATIO)\n",
    "            \n",
    "            # for training data\n",
    "            file_no = 0\n",
    "            while file_no < no_file_train : \n",
    "                selectedFile = random.choice(os.listdir(tmp_dir + '/' + species.name))\n",
    "                no = selectedFile.find('_s')\n",
    "                no+=2\n",
    "                while selectedFile[no:no+1].isnumeric():\n",
    "                    no+=1                \n",
    "                selectedFile = selectedFile[:no] + '*'\n",
    "                #print('Selected ' + selectedFile)\n",
    "                file_moved = 0\n",
    "                for file in glob.iglob(class_dir +'/'+ selectedFile, recursive=True):    \n",
    "                    file_name = os.path.basename(file)\n",
    "                    shutil.move(file, strain_dir + file_name)\n",
    "                    file_moved += 1\n",
    "                # sum file_moved to file_no    \n",
    "                file_no += file_moved\n",
    "            print('No. of files in \"' + strain_dir + '\" should be around ' + str(no_file_train) + '.')\n",
    "            print('No. of files in ' +strain_dir + ' = ' + str(file_no) +'.' )\n",
    "            \n",
    "            # for test data\n",
    "            file_no = 0\n",
    "            while file_no < no_file_test and os.listdir(tmp_dir + '/' + species.name): \n",
    "                selectedFile = random.choice(os.listdir(tmp_dir + '/' + species.name))\n",
    "                #print(selectedFile)\n",
    "                no = selectedFile.find('_s')\n",
    "                no+=2\n",
    "                while selectedFile[no:no+1].isnumeric():\n",
    "                    no+=1\n",
    "                selectedFile = selectedFile[:no] + '*'\n",
    "                #print('Selected ' + selectedFile)\n",
    "                file_moved = 0\n",
    "                for file in glob.iglob(class_dir +'/'+ selectedFile, recursive=True):    \n",
    "                    file_name = os.path.basename(file)\n",
    "                    shutil.move(file, strain_dir + file_name)\n",
    "                    file_moved += 1\n",
    "                # sum file_moved to file_no    \n",
    "                file_no += file_moved\n",
    "            print('No. of files in \"' + stest_dir + '\" should be around ' + str(no_file_test) + '.')\n",
    "            print('No. of files in ' +stest_dir + ' = ' + str(file_no) +'.' )\n",
    "            #move all remaining files to val_dir\n",
    "            selectedFile = '*'\n",
    "            file_moved = 0\n",
    "            for file in glob.iglob(class_dir +'/'+ selectedFile, recursive=True):    \n",
    "                file_name = os.path.basename(file)\n",
    "                shutil.move(file, sval_dir + file_name)\n",
    "                file_moved += 1\n",
    "            file_no = file_moved    \n",
    "            print('No. of files in \"' + sval_dir + '\" should be around ' + str(no_file_val) + '.')\n",
    "            print('No. of files in ' +sval_dir + ' = ' + str(file_no) +'.' )\n",
    "                \n",
    "\n",
    "def SplitLeftRight(src_dir, dst_dir):\n",
    "    if not os.path.exists(src_dir):\n",
    "        print('Source directory does not exist!')\n",
    "        return\n",
    "    if os.path.exists(dst_dir):\n",
    "        print('Destination directory already exists.  Do nothing!!!')\n",
    "        return\n",
    "    trainS = src_dir + '/train/'    \n",
    "    valS = src_dir + '/val/'\n",
    "    testS = src_dir + '/test/'\n",
    "    LEFT = '/Left/'\n",
    "    RIGHT = '/Right/'\n",
    "    src_subdir = ('train', 'val', 'test')\n",
    "\n",
    "    if not os.path.exists(trainS) or not os.path.exists(valS) or not os.path.exists(testS):\n",
    "        print('Source directory must contains all [\"train\", \"val\", \"test\"] subdirectories')\n",
    "        return\n",
    "    # copy source dir to tmp\n",
    "    tmp = tempfile.TemporaryDirectory()\n",
    "    tmp_dir = tmp.name + '/tmp/'\n",
    "    shutil.copytree(src_dir, tmp_dir)\n",
    "\n",
    "    # mkdir of destination structure\n",
    "    os.mkdir(dst_dir)\n",
    "    os.mkdir(dst_dir + LEFT)\n",
    "    os.mkdir(dst_dir + RIGHT)\n",
    "    \n",
    "    # scan sub\n",
    "    subidx = 0\n",
    "    files_in_train_val_test = [0,0,0]\n",
    "    for sub in src_subdir: # ('train', 'val', 'test')\n",
    "        # create LEFT, RIGHT sub\n",
    "        dst_left_sub = dst_dir + LEFT + sub\n",
    "        dst_right_sub = dst_dir + RIGHT + sub\n",
    "        os.mkdir(dst_left_sub)\n",
    "        os.mkdir(dst_right_sub)\n",
    "        # scan species\n",
    "        for species in os.scandir(tmp_dir + sub):\n",
    "            if not species.name.startswith('.') and species.is_dir():\n",
    "                # count total files\n",
    "                file_no = 0\n",
    "                for entry2 in os.scandir(species):\n",
    "                    if not entry2.name.startswith('.') and entry2.is_file():\n",
    "                        file_no +=1\n",
    "                print('There are ' + str(file_no)  + ' files in SRC_DIR/'+ sub + '/' + species.name +'.')\n",
    "                # half of it\n",
    "                half_file_no = math.floor(file_no/2)\n",
    "                print('There should be ' + str(half_file_no)  + ' files in DST_DIR/[LEFT,RIGHT]/'+ sub + '/' + species.name +'.')\n",
    "                print('.........')\n",
    "                # get the file list\n",
    "                FileList = sorted( filter( os.path.isfile, glob.glob(tmp_dir + sub + '/' + species.name + '/*') ) )\n",
    "                # devide by half\n",
    "                FileListL = FileList[0:half_file_no]\n",
    "                FileListR = FileList[half_file_no:half_file_no*2]\n",
    "                files_in_train_val_test[subidx] += half_file_no\n",
    "                \n",
    "                # create species directory\n",
    "                dst_left_sub_species = dst_left_sub + '/' + species.name \n",
    "                dst_right_sub_species = dst_right_sub + '/' + species.name \n",
    "                os.mkdir(dst_left_sub_species)\n",
    "                os.mkdir(dst_right_sub_species)\n",
    "                for file in FileListL:\n",
    "                    file_name = os.path.basename(file)\n",
    "                    shutil.move(file, dst_left_sub_species + '/' + file_name)\n",
    "                for file in FileListR:\n",
    "                    file_name = os.path.basename(file)\n",
    "                    shutil.move(file, dst_right_sub_species + '/' + file_name)\n",
    "        subidx += 1\n",
    "    print('There are ' +str(files_in_train_val_test[0]) + ' files in train, ' \n",
    "          + str(files_in_train_val_test[1]) + ' files in val ' \n",
    "          + str(files_in_train_val_test[2]) + ' files in test.')\n",
    "    return (files_in_train_val_test[0], files_in_train_val_test[1], files_in_train_val_test[2])\n",
    "\n",
    "def SplitLeftMiddleRight(src_dir, dst_dir):\n",
    "    if not os.path.exists(src_dir):\n",
    "        print('Source directory does not exist!')\n",
    "        return\n",
    "    if os.path.exists(dst_dir):\n",
    "        print('Destination directory already exists.  Do nothing!!!')\n",
    "        return\n",
    "    trainS = src_dir + '/train/'    \n",
    "    valS = src_dir + '/val/'\n",
    "    testS = src_dir + '/test/'\n",
    "    LEFT = '/Left/'\n",
    "    MIDDLE = '/Middle/'\n",
    "    RIGHT = '/Right/'\n",
    "    src_subdir = ('train', 'val', 'test')\n",
    "\n",
    "    if not os.path.exists(trainS) or not os.path.exists(valS) or not os.path.exists(testS):\n",
    "        print('Source directory must contains all [\"train\", \"val\", \"test\"] subdirectories')\n",
    "        return\n",
    "    # copy source dir to tmp\n",
    "    tmp = tempfile.TemporaryDirectory()\n",
    "    tmp_dir = tmp.name + '/tmp/'\n",
    "    shutil.copytree(src_dir, tmp_dir)\n",
    "\n",
    "    # mkdir of destination structure\n",
    "    os.mkdir(dst_dir)\n",
    "    os.mkdir(dst_dir + LEFT)\n",
    "    os.mkdir(dst_dir + MIDDLE)\n",
    "    os.mkdir(dst_dir + RIGHT)\n",
    "    \n",
    "    # scan sub\n",
    "    subidx = 0\n",
    "    files_in_train_val_test = [0,0,0]\n",
    "    for sub in src_subdir: # ('train', 'val', 'test')\n",
    "        # create LEFT, RIGHT sub\n",
    "        dst_left_sub = dst_dir + LEFT + sub\n",
    "        dst_middle_sub = dst_dir + MIDDLE + sub\n",
    "        dst_right_sub = dst_dir + RIGHT + sub\n",
    "        os.mkdir(dst_left_sub)\n",
    "        os.mkdir(dst_middle_sub)\n",
    "        os.mkdir(dst_right_sub)\n",
    "        # scan species\n",
    "        for species in os.scandir(tmp_dir + sub):\n",
    "            if not species.name.startswith('.') and species.is_dir():\n",
    "                # count total files\n",
    "                file_no = 0\n",
    "                for entry2 in os.scandir(species):\n",
    "                    if not entry2.name.startswith('.') and entry2.is_file():\n",
    "                        file_no +=1\n",
    "                print('There are ' + str(file_no)  + ' files in SRC_DIR/'+ sub + '/' + species.name +'.')\n",
    "                # third of it\n",
    "                third_file_no = math.floor(file_no/3)\n",
    "                print('There should be ' + str(third_file_no)  + ' files in DST_DIR/[LEFT,MIDDLE,RIGHT]/'+ sub + '/' + species.name +'.')\n",
    "                print('.........')\n",
    "                # get the file list\n",
    "                FileList = sorted( filter( os.path.isfile, glob.glob(tmp_dir + sub + '/' + species.name + '/*') ) )\n",
    "                # devide by half\n",
    "                FileListL = FileList[0:third_file_no]\n",
    "                FileListM = FileList[third_file_no:third_file_no*2]\n",
    "                FileListR = FileList[third_file_no*2:third_file_no*3]\n",
    "                files_in_train_val_test[subidx] += third_file_no\n",
    "                \n",
    "                # create species directory\n",
    "                dst_left_sub_species = dst_left_sub + '/' + species.name \n",
    "                dst_middle_sub_species = dst_middle_sub + '/' + species.name\n",
    "                dst_right_sub_species = dst_right_sub + '/' + species.name \n",
    "                os.mkdir(dst_left_sub_species)\n",
    "                os.mkdir(dst_middle_sub_species)\n",
    "                os.mkdir(dst_right_sub_species)\n",
    "                for file in FileListL:\n",
    "                    file_name = os.path.basename(file)\n",
    "                    shutil.move(file, dst_left_sub_species + '/' + file_name)\n",
    "                for file in FileListM:\n",
    "                    file_name = os.path.basename(file)\n",
    "                    shutil.move(file, dst_middle_sub_species + '/' + file_name)\n",
    "                for file in FileListR:\n",
    "                    file_name = os.path.basename(file)\n",
    "                    shutil.move(file, dst_right_sub_species + '/' + file_name)\n",
    "        subidx += 1\n",
    "    print('There are ' +str(files_in_train_val_test[0]) + ' files in train, ' \n",
    "          + str(files_in_train_val_test[1]) + ' files in val ' \n",
    "          + str(files_in_train_val_test[2]) + ' files in test.')\n",
    "    return (files_in_train_val_test[0], files_in_train_val_test[1], files_in_train_val_test[2])\n",
    " \n",
    "\n",
    "def randomSplitMosData(input_dir, output_dirLR, output_dirLMR, ratio = RATIO):\n",
    "    tmpX = tempfile.TemporaryDirectory().name\n",
    "    RandomTrainValTest(input_dir,tmpX, ratio = ratio)\n",
    "    #tmpX = '/tmp/tmpk_mjnf4g'\n",
    "    no_files_LR = SplitLeftRight(tmpX, output_dirLR)\n",
    "    no_files_LMR = SplitLeftMiddleRight(tmpX, output_dirLMR)\n",
    "    return no_files_LR, no_files_LMR    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0fba398f-6fbb-4e9b-98be-3c8368749fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_ds(src_dir):\n",
    "    \n",
    "    train_dir = src_dir  + '/Left/train'\n",
    "    train_dir1 = src_dir + '/Right/train'\n",
    "    val_dir = src_dir    + '/Left/val'\n",
    "    val_dir1 = src_dir   + '/Right/val'\n",
    "    \n",
    "    train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "      train_dir,\n",
    "      #validation_split=0.2,\n",
    "      #subset=\"training\",\n",
    "      seed=123,\n",
    "      image_size=(IMG_SIZE, IMG_SIZE),\n",
    "      shuffle = False,\n",
    "      label_mode='categorical',\n",
    "      batch_size=BATCH_SIZE)\n",
    "\n",
    "    # val_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    #   val_dir,\n",
    "    #   #validation_split=0.2,\n",
    "    #   #subset=\"validation\",\n",
    "    #   seed=123,\n",
    "    #   shuffle = False,\n",
    "    #   label_mode='categorical',\n",
    "    #   image_size=(IMG_SIZE, IMG_SIZE),\n",
    "    #   batch_size=BATCH_SIZE)\n",
    "\n",
    "\n",
    "\n",
    "    train_ds1 = tf.keras.utils.image_dataset_from_directory(\n",
    "      train_dir1,\n",
    "      #validation_split=0.2,\n",
    "      #subset=\"training\",\n",
    "      seed=123,\n",
    "      shuffle = False,\n",
    "      label_mode='categorical',\n",
    "      image_size=(IMG_SIZE, IMG_SIZE),\n",
    "      batch_size=BATCH_SIZE)\n",
    "    \n",
    "    val_ds = val_ds1 = 0\n",
    "\n",
    "    # val_ds1 = tf.keras.utils.image_dataset_from_directory(\n",
    "    #   val_dir1,\n",
    "    #   #validation_split=0.2,\n",
    "    #   #subset=\"validation\",\n",
    "    #   seed=123,\n",
    "    #   shuffle = False,\n",
    "    #   label_mode='categorical',\n",
    "    #   image_size=(IMG_SIZE, IMG_SIZE),\n",
    "    #   batch_size=BATCH_SIZE)\n",
    "    \n",
    "    return (train_ds, val_ds, train_ds1, val_ds1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83a994dd-3511-42b0-83f5-bce39e3d6863",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combineDS(train_ds, val_ds, train_ds1, val_ds1):\n",
    "    # Rescale all datasets\n",
    "    normalization_layer = tf.keras.layers.Rescaling(1./255)\n",
    "    train_ds = train_ds.map(lambda x, y: (normalization_layer(x), y), \n",
    "                    num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    # val_ds = val_ds.map(lambda x, y: (normalization_layer(x), y), \n",
    "    #                 num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "    train_ds1 = train_ds1.map(lambda x, y: (normalization_layer(x), y), \n",
    "                    num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    # val_ds1 = val_ds1.map(lambda x, y: (normalization_layer(x), y), \n",
    "    #                 num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "    train_ds_X = train_ds.map(lambda x,y:(x), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    train_ds_X1 = train_ds1.map(lambda x,y:(x), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    train_ds_Y = train_ds.map(lambda x,y:(y), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "\n",
    "    # val_ds_X = val_ds.map(lambda x,y:(x), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    # val_ds_X1 = val_ds1.map(lambda x,y:(x), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    # val_ds_Y = val_ds.map(lambda x,y:(y), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    \n",
    "    # combine left, right \n",
    "    train_ds_X_X1 = tf.data.Dataset.zip((train_ds_X, train_ds_X1))\n",
    "\n",
    "\n",
    "    # attach label back\n",
    "    train_ds_X_X1_Y = tf.data.Dataset.zip((train_ds_X_X1, train_ds_Y))\n",
    "\n",
    "    #--------------------------------------------------------------\n",
    "\n",
    "    val_ds_X_X1_Y = 0\n",
    "   \n",
    "    \n",
    "    return (train_ds_X_X1_Y, val_ds_X_X1_Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5616ebfb-1ba0-4baf-987f-6b3762313fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dsLMR(LMR_dir):\n",
    "    tmp_dir = LMR_dir\n",
    "    tmp_dirL    = tmp_dir + '/Left/'\n",
    "    tmp_dirM    = tmp_dir + '/Middle/'\n",
    "    tmp_dirR    = tmp_dir + '/Right/'\n",
    "    # for repeatable train, val, test folder spliting\n",
    "    now = datetime.now()\n",
    "    SEED = 123409 + np.random.randint(-100000,100000)\n",
    "\n",
    "    train_dirL   = tmp_dirL + 'train'\n",
    "    train_dirM  = tmp_dirM + 'train'\n",
    "    train_dirR  = tmp_dirR + 'train'\n",
    "    test_dirL    = tmp_dirL + 'val'\n",
    "    test_dirM   = tmp_dirM + 'val'\n",
    "    test_dirR   = tmp_dirR + 'val'\n",
    "\n",
    "    train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "        train_dirL,\n",
    "        seed=SEED,\n",
    "        image_size=(IMG_SIZE, IMG_SIZE),\n",
    "        shuffle = False,\n",
    "        label_mode='categorical',\n",
    "        crop_to_aspect_ratio = True,\n",
    "        batch_size=BATCH_SIZE)\n",
    "\n",
    "\n",
    "    # val_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    #     test_dirL,\n",
    "    #     seed=SEED,\n",
    "    #     shuffle = False,\n",
    "    #     label_mode='categorical',\n",
    "    #     image_size=(IMG_SIZE, IMG_SIZE),\n",
    "    #     batch_size=BATCH_SIZE)\n",
    "\n",
    "\n",
    "    train_ds1 = tf.keras.utils.image_dataset_from_directory(\n",
    "        train_dirM,\n",
    "        seed=SEED,\n",
    "        shuffle = False,\n",
    "        label_mode='categorical',\n",
    "        image_size=(IMG_SIZE, IMG_SIZE),\n",
    "        crop_to_aspect_ratio = True,\n",
    "        batch_size=BATCH_SIZE)\n",
    "\n",
    "\n",
    "    # val_ds1 = tf.keras.utils.image_dataset_from_directory(\n",
    "    #     test_dirM,\n",
    "    #     seed=SEED,\n",
    "    #     shuffle = False,\n",
    "    #     label_mode='categorical',\n",
    "    #     image_size=(IMG_SIZE, IMG_SIZE),\n",
    "    #     batch_size=BATCH_SIZE)\n",
    "\n",
    "    train_ds2 = tf.keras.utils.image_dataset_from_directory(\n",
    "        train_dirR,\n",
    "        seed=SEED,\n",
    "        shuffle = False,\n",
    "        label_mode='categorical',\n",
    "        image_size=(IMG_SIZE, IMG_SIZE),\n",
    "        crop_to_aspect_ratio = True,\n",
    "        batch_size=BATCH_SIZE)\n",
    "\n",
    "\n",
    "    # val_ds2 = tf.keras.utils.image_dataset_from_directory(\n",
    "    #     test_dirR,\n",
    "    #     seed=SEED,\n",
    "    #     shuffle = False,\n",
    "    #     label_mode='categorical',\n",
    "    #     image_size=(IMG_SIZE, IMG_SIZE),\n",
    "    #     batch_size=BATCH_SIZE)\n",
    "    val_ds = val_ds1 = val_ds2 = 0\n",
    "    return (train_ds, val_ds, train_ds1, val_ds1, train_ds2, val_ds2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c7ccf277-f50c-40e2-a58d-79d4143d2010",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combineDS_LMR(train_ds, val_ds, train_ds1, val_ds1, train_ds2, val_ds2):\n",
    "    # Rescale all datasets\n",
    "    normalization_layer = tf.keras.layers.Rescaling(1./255)\n",
    "    train_ds = train_ds.map(lambda x, y: (normalization_layer(x), y), \n",
    "                    num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    # val_ds = val_ds.map(lambda x, y: (normalization_layer(x), y), \n",
    "    #                 num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "    train_ds1 = train_ds1.map(lambda x, y: (normalization_layer(x), y), \n",
    "                    num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    # val_ds1 = val_ds1.map(lambda x, y: (normalization_layer(x), y), \n",
    "    #                 num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "    train_ds2 = train_ds2.map(lambda x, y: (normalization_layer(x), y), \n",
    "                    num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    # val_ds2 = val_ds2.map(lambda x, y: (normalization_layer(x), y), \n",
    "    #                 num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    train_ds_X = train_ds.map(lambda x,y:(x), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    train_ds_X1 = train_ds1.map(lambda x,y:(x), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    train_ds_X2 = train_ds2.map(lambda x,y:(x), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    train_ds_Y = train_ds.map(lambda x,y:(y), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "\n",
    "    # val_ds_X = val_ds.map(lambda x,y:(x), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    # val_ds_X1 = val_ds1.map(lambda x,y:(x), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    # val_ds_X2 = val_ds2.map(lambda x,y:(x), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    # val_ds_Y = val_ds.map(lambda x,y:(y), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    \n",
    "    # combine left, right dataset 6 combinations\n",
    "    train_ds_X_X1 = tf.data.Dataset.zip((train_ds_X, train_ds_X1))\n",
    "\n",
    "    train_ds_X1_X = tf.data.Dataset.zip((train_ds_X1, train_ds_X))\n",
    "\n",
    "    train_ds_X1_X2 = tf.data.Dataset.zip((train_ds_X1, train_ds_X2))\n",
    "\n",
    "    train_ds_X2_X1 = tf.data.Dataset.zip((train_ds_X2, train_ds_X1))\n",
    "\n",
    "    train_ds_X_X2 = tf.data.Dataset.zip((train_ds_X, train_ds_X2))\n",
    "\n",
    "    train_ds_X2_X = tf.data.Dataset.zip((train_ds_X2, train_ds_X))\n",
    "\n",
    "    #--------------------------------------------------------------\n",
    "#     val_ds_X_X1 = tf.data.Dataset.zip((val_ds_X, val_ds_X1))\n",
    "\n",
    "#     val_ds_X1_X = tf.data.Dataset.zip((val_ds_X1, val_ds_X))\n",
    "\n",
    "#     val_ds_X1_X2 = tf.data.Dataset.zip((val_ds_X1, val_ds_X2))\n",
    "\n",
    "#     val_ds_X2_X1 = tf.data.Dataset.zip((val_ds_X2, val_ds_X1))\n",
    "\n",
    "#     val_ds_X_X2 = tf.data.Dataset.zip((val_ds_X, val_ds_X2))\n",
    "\n",
    "#     val_ds_X2_X = tf.data.Dataset.zip((val_ds_X2, val_ds_X))\n",
    "    \n",
    "    # attach label back\n",
    "    train_ds_XL_XM_Y = tf.data.Dataset.zip((train_ds_X_X1, train_ds_Y))\n",
    "    train_ds_XM_XL_Y = tf.data.Dataset.zip((train_ds_X1_X, train_ds_Y))\n",
    "    train_ds_XM_XR_Y = tf.data.Dataset.zip((train_ds_X1_X2, train_ds_Y))\n",
    "    train_ds_XR_XM_Y = tf.data.Dataset.zip((train_ds_X2_X1, train_ds_Y))\n",
    "    train_ds_XL_XR_Y = tf.data.Dataset.zip((train_ds_X_X2, train_ds_Y))\n",
    "    train_ds_XR_XL_Y = tf.data.Dataset.zip((train_ds_X2_X, train_ds_Y))\n",
    "\n",
    "    #--------------------------------------------------------------\n",
    "    # val_ds_XL_XM_Y = tf.data.Dataset.zip((val_ds_X_X1, val_ds_Y))\n",
    "    # val_ds_XM_XL_Y = tf.data.Dataset.zip((val_ds_X1_X, val_ds_Y))\n",
    "    # val_ds_XM_XR_Y = tf.data.Dataset.zip((val_ds_X1_X2, val_ds_Y))\n",
    "    # val_ds_XR_XM_Y = tf.data.Dataset.zip((val_ds_X2_X1, val_ds_Y))\n",
    "    # val_./models/Model_Early_Combine_all_Vivo-V21_2022-10-22_01-04.h5./models/Model_Early_Combine_all_Vivo-V21_2022-10-22_01-04.h5./models/Model_Early_Combine_all_Vivo-V21_2022-10-22_01-04.h5ds_XL_XR_Y = tf.data.Dataset.zip((val_ds_X_X2, val_ds_Y))\n",
    "    # val_ds_XR_XL_Y = tf.data.Dataset.zip((val_ds_X2_X, val_ds_Y))\n",
    "\n",
    "    val_ds_XL_XM_Y = 0\n",
    "    val_ds_XM_XL_Y = 0\n",
    "    val_ds_XM_XR_Y = 0\n",
    "    val_ds_XR_XM_Y = 0\n",
    "    val_ds_XL_XR_Y = 0\n",
    "    val_ds_XR_XL_Y = 0\n",
    "    \n",
    "    \n",
    "    return (train_ds_XL_XM_Y, val_ds_XL_XM_Y,\n",
    "            train_ds_XM_XL_Y, val_ds_XM_XL_Y,\n",
    "            train_ds_XM_XR_Y, val_ds_XM_XR_Y,\n",
    "            train_ds_XR_XM_Y, val_ds_XR_XM_Y,\n",
    "            train_ds_XL_XR_Y, val_ds_XL_XR_Y,\n",
    "            train_ds_XL_XM_Y, val_ds_XL_XM_Y,\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2d00c801-b667-44e9-a153-926d6437b7ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictLMR(model, train_ds_XL_XM_Y, val_ds_XL_XM_Y,\n",
    "            train_ds_XM_XL_Y, val_ds_XM_XL_Y,\n",
    "            train_ds_XM_XR_Y, val_ds_XM_XR_Y,\n",
    "            train_ds_XR_XM_Y, val_ds_XR_XM_Y,\n",
    "            train_ds_XL_XR_Y, val_ds_XL_XR_Y,\n",
    "            train_ds_XR_XL_Y, val_ds_XR_XL_Y):\n",
    "    Y_pred = model.predict(train_ds_XL_XM_Y)\n",
    "    Y_pred = np.hstack((Y_pred, model.predict(train_ds_XM_XL_Y)))\n",
    "    Y_pred = np.hstack((Y_pred, model.predict(train_ds_XM_XR_Y)))\n",
    "    Y_pred = np.hstack((Y_pred, model.predict(train_ds_XR_XM_Y)))\n",
    "    Y_pred = np.hstack((Y_pred, model.predict(train_ds_XL_XR_Y)))\n",
    "    Y_pred = np.hstack((Y_pred, model.predict(train_ds_XR_XL_Y)))\n",
    "\n",
    "    # Ypred1 = model.predict(val_ds_XL_XM_Y)\n",
    "    # Ypred1 = np.hstack((Ypred1, model.predict(val_ds_XM_XL_Y)))\n",
    "    # Ypred1 = np.hstack((Ypred1, model.predict(val_ds_XM_XR_Y)))\n",
    "    # Ypred1 = np.hstack((Ypred1, model.predict(val_ds_XR_XM_Y)))\n",
    "    # Ypred1 = np.hstack((Ypred1, model.predict(val_ds_XL_XR_Y)))\n",
    "    # Ypred1 = np.hstack((Ypred1, model.predict(val_ds_XR_XL_Y)))\n",
    "    Ypred1 = 0\n",
    "    return (Y_pred, Ypred1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8b0abada-55ec-4b8a-bf45-4028f5a7ed71",
   "metadata": {},
   "outputs": [],
   "source": [
    "earlyModelPath=\"./models/Model_Early_Combine_all_Vivo-V21_2022-10-22_01-04.h5\"\n",
    "middleModelPath=\"./models/Model_Middle_Combine_all_Vivo-V21_2022-10-22_07-50.h5\"\n",
    "lateModelPath=\"./models/Model_Late_Combine_all_Vivo-V21_2022-10-22_09-23.h5\"\n",
    "ensemblePath=\"./models/Ensemble_all_Vivo-V21_2022-10-22_12-45.h5\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
